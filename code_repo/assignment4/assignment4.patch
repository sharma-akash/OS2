--- syscalls_old.h	2017-10-05 19:11:04.000000000 -0700
+++ syscalls.h	2017-12-03 22:24:58.000000000 -0800
@@ -882,4 +882,9 @@
 			const char __user *const __user *argv,
 			const char __user *const __user *envp, int flags);

+asmlinkage long sys_bf_slob_space_used(void);
+
+asmlinkage long sys_bf_slob_space_free(void);
+
+
--- syscall_32_old.tbl	2017-10-05 19:11:00.000000000 -0700
+++ syscall_32.tbl	2017-12-03 22:27:40.000000000 -0800
@@ -365,3 +365,5 @@
 356	i386	memfd_create		sys_memfd_create
 357	i386	bpf			sys_bpf
 358	i386	execveat		sys_execveat			stub32_execveat
+359 i386  bf_slob_space_free sys_bf_slob_space_free
+360 i386  bf_slob_space_used sys_bf_slob_space_used
--- slob_old.c	2017-10-05 19:11:04.000000000 -0700
+++ slob.c	2017-12-03 22:47:11.000000000 -0800
@@ -1,4 +1,4 @@
-/*
+++/*
  * SLOB Allocator: Simple List Of Blocks
  *
  * Matt Mackall <mpm@selenic.com> 12/30/03
@@ -72,6 +72,9 @@

 #include <linux/atomic.h>

+#include <linux/linkage.h>
+#include <linux/syscalls.h>
+
 #include "slab.h"
 /*
  * slob_block has a field 'units', which indicates size of block if +ve,
@@ -87,6 +90,12 @@
 typedef s32 slobidx_t;
 #endif

+#define BEST_FIT //comment out to do first_fit slob
+
+long free_space = 0; //variable to keep track of free space
+int num_pages = 0; //variable to keep track of number of pages
+
+
 struct slob_block {
 	slobidx_t units;
 };
@@ -268,11 +277,13 @@
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct page *sp;
+	struct page *best_fit_sp = sp;
 	struct list_head *prev;
 	struct list_head *slob_list;
 	slob_t *b = NULL;
 	unsigned long flags;

+
 	if (size < SLOB_BREAK1)
 		slob_list = &free_slob_small;
 	else if (size < SLOB_BREAK2)
@@ -295,20 +306,47 @@
 		if (sp->units < SLOB_UNITS(size))
 			continue;

+
+#ifdef BEST_FIT
+		if (sp->units < best_fit_sp->units) {
+			best_fit_sp = sp; //if sp is a better fit than current best fit
+#else
 		/* Attempt to alloc */
 		prev = sp->lru.prev;
 		b = slob_page_alloc(sp, size, align);
 		if (!b)
 			continue;
+#endif
+
+		}

 		/* Improve fragment distribution and reduce our average
 		 * search time by starting our next search here. (see
 		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
+		 //if (prev != slob_list->prev &&
+		//		slob_list->next != prev->next)
+		//	list_move_tail(slob_list, prev->next);
+		// break;
 	}
+
+#ifdef BEST_FIT
+	b = slob_page_alloc(best_fit_sp, size, align);
+#endif
+
+	/* Compute free space */
+
+	list_for_each_entry(sp, &free_slob_small, lru) {
+		free_space += sp->units;
+	}
+
+	list_for_each_entry(sp, &free_slob_medium, lru) {
+		free_space += sp->units;
+	}
+
+	list_for_each_entry(sp, &free_slob_large, lru) {
+		free_space += sp->units;
+	}
+
 	spin_unlock_irqrestore(&slob_lock, flags);

 	/* Not enough space: must allocate a new page */
@@ -328,6 +366,7 @@
 		b = slob_page_alloc(sp, size, align);
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
+		num_pages += 1;
 	}
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
@@ -362,6 +401,7 @@
 		__ClearPageSlab(sp);
 		page_mapcount_reset(sp);
 		slob_free_pages(b, 0);
+		num_pages -= 1;
 		return;
 	}

@@ -630,6 +670,14 @@
 	.align = ARCH_KMALLOC_MINALIGN,
 };

+asmlinkage long sys_bf_slob_space_used(void) {
+	return SLOB_UNITS(PAGE_SIZE) * num_pages;
+}
+
+asmlinkage long sys_bf_slob_space_free(void) {
+	return free_units;
+}
+
 void __init kmem_cache_init(void)
 {
 	kmem_cache = &kmem_cache_boot;
